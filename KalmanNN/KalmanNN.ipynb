{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# work with data\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# like numpy, only good + NN\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as torch_nn_F\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "# import torchvision\n",
    "# from torchvision import transforms\n",
    "\n",
    "plt.rcParams[\n",
    "    \"figure.facecolor\"\n",
    "] = \"w\"  # force white background on plots when using dark mode in JupyterLab\n",
    "\n",
    "# Dark plots\n",
    "#plt.style.use('dark_background')\n",
    "    \n",
    "TIME_STEP = 1\n",
    "CLASSNAME = {0: 'ballistic', 1: 'hgv', 2: 'hcm'}\n",
    "CLASSTYPE = {'ballistic': 0, 'hgv': 1, 'hcm': 2}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data\n",
    "## Load and examine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data reading function\n",
    "def read_trajectories(path_to_file): \n",
    "    dset = pd.HDFStore(path_to_file, 'r')\n",
    "#     print(dset.info())\n",
    "    return dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading\n",
    "name_file_trajectories = './data/ballistic_batch_1000_nopad.h5'\n",
    "try:\n",
    "    dset = read_trajectories(name_file_trajectories)\n",
    "except FileNotFoundError:\n",
    "    assert \"NO file!!!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(data, tts = (0.7, 0.1, 0.2), shuffle=False):\n",
    "    '''Split data into train, validation and test sets according to `tts` tuple\n",
    "    \n",
    "    By default, tts = (train, val, test) = (0.7, 0.1, 0.2)\n",
    "    '''\n",
    "    assert sum(tts) == 1\n",
    "    \n",
    "    if shuffle:\n",
    "        data = np.random.shuffle(data)\n",
    "    \n",
    "    h = len(data)\n",
    "    train = data[:int(h * tts[0])]\n",
    "    val = data[int(h * tts[0]) : int(h * np.round(tts[0] + tts[1], 4))]\n",
    "    test = data[int(h * np.round(tts[0] + tts[1], 4)) : int(h * sum(tts))]\n",
    "\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train keys:   700 -> 70.0%\n",
      "Valid keys:   100 -> 10.0%\n",
      "Test keys:    200 -> 20.0%\n"
     ]
    }
   ],
   "source": [
    "# Train-val-test split data\n",
    "data_keys = sorted([key for key in dset.keys() if 'raw' in key])\n",
    "# Test data       = 20% of total keys\n",
    "# Validation data = 10% of total keys\n",
    "# Train data      = 70% of total keys\n",
    "train_keys, val_keys, test_keys = train_val_test_split(\n",
    "    data_keys, (0.7, 0.1, 0.2), shuffle=False)\n",
    "\n",
    "print(f'Train keys: {len(train_keys):>5} -> {len(train_keys)/len(data_keys) * 100}%')\n",
    "print(f'Valid keys: {len(val_keys):>5} -> {len(val_keys)/len(data_keys) * 100}%')\n",
    "print(f'Test keys:  {len(test_keys):>5} -> {len(test_keys)/len(data_keys) * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example\n",
    "# len(dset.keys())\n",
    "# dset[dset.keys()[0]]\n",
    "# len(np.array(dset[dset.keys()[0]])[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invertible trajectory pre-processing transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeTrajectory():\n",
    "    def __init__(self, df, diff=False, ref_point=20):\n",
    "\n",
    "        self.diff = diff\n",
    "        self.ref_point = ref_point\n",
    "        \n",
    "        data = np.array(df.drop('t', 1))\n",
    "        \n",
    "        # Remember first state\n",
    "        self.start_state = data[0].copy()\n",
    "        \n",
    "        # Make changes relative to the start state\n",
    "        data -= self.start_state\n",
    "        \n",
    "        # Rotate coordinate system around Z axis, \n",
    "        # so X` axis will pass through the 20'th point\n",
    "        # and Y` axis will represent deviation\n",
    "\n",
    "        ## Calculate angle of rotation: arctan(y_r / x_r), where r is ref_point\n",
    "        assert data.shape[0] >= ref_point\n",
    "        self.theta = np.arctan(data[ref_point][1] / data[ref_point][0])\n",
    "\n",
    "        ## Rotation matrix for XY plane around Z axis\n",
    "        ## Perform rotation for coordinates\n",
    "        data[:, :3] = self.rotate_Z(data[:, :3], self.theta)\n",
    "\n",
    "        ## Perform rotation for velocities\n",
    "        data[:, 3:6] = self.rotate_Z(data[:, 3:6], self.theta)\n",
    "\n",
    "        # Scale data to kilometers\n",
    "        data /= 1000\n",
    "\n",
    "        self.first_diff_elem = None\n",
    "        if diff:\n",
    "            self.first_diff_elem = data[0].copy()\n",
    "            data = np.diff(data, axis=0)    \n",
    "        \n",
    "        self.data = data\n",
    "        \n",
    "    def restore(self, columns, ts=1, config=None):\n",
    "        \n",
    "        assert ts > 0\n",
    "        \n",
    "        if config:\n",
    "            self.set_info(config)\n",
    "        \n",
    "        # Restore diff data\n",
    "        if self.diff:\n",
    "            data = np.r_[np.expand_dims(self.first_diff_elem, 0), np.cumsum(self.data, axis=0)]\n",
    "        else:         \n",
    "            data = self.data.copy()\n",
    "        \n",
    "        # Scale data from km back to meters\n",
    "        data *= 1000\n",
    "        \n",
    "        ## Rotation matrix for XY plane around Z axis\n",
    "        ## Perform rotation for coordinates\n",
    "        data[:, 0:3] = self.rotate_Z(data[:, 0:3], -self.theta)\n",
    "\n",
    "        ## Perform rotation for velocities\n",
    "        data[:, 3:6] = self.rotate_Z(data[:, 3:6], -self.theta)\n",
    "        \n",
    "        # Make changes absolute\n",
    "        data += self.start_state\n",
    "\n",
    "        # Restore Pandas.DataFrame format\n",
    "        t = np.arange(0, data.shape[0], ts)\n",
    "        data = np.c_[data, t] \n",
    "        data = pd.DataFrame(data, columns=columns)\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    @staticmethod\n",
    "    def rotate_Z(data, theta):\n",
    "        \"\"\"Rotate data around the Z axis using matrix R\"\"\"\n",
    "        \n",
    "        R = np.array([\n",
    "            [np.cos(theta), -np.sin(theta), 0],\n",
    "            [np.sin(theta),  np.cos(theta), 0],\n",
    "            [            0,              0, 1]\n",
    "        ])\n",
    "        return data @ R.T\n",
    "    \n",
    "    def info(self):\n",
    "        return {\n",
    "            'ref_point' : self.ref_point,\n",
    "            'diff' : self.diff,\n",
    "            'start_state' : self.start_state,\n",
    "            'first_diff_elem' : self.first_diff_elem,\n",
    "            'theta' : self.theta\n",
    "        }\n",
    "    \n",
    "    def set_info(self, config):\n",
    "        self.ref_point = config['ref_point']\n",
    "        self.diff = config['diff']\n",
    "        self.start_point = config['start_point']\n",
    "        self.first_diff_elem = config['first_diff_elem']\n",
    "        self.theta = config['theta']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_len_coordinate(dset, keys):\n",
    "    max_size = 0\n",
    "    for key_k in keys:\n",
    "        size = len(np.array(dset[key_k])[:,0])\n",
    "        if size > max_size:\n",
    "            max_size = size\n",
    "            key = key_k\n",
    "    return (max_size, key)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_strided_data_clust(dset, keys, variables=3, gt_size=0, step=1, diff=False, verbose=False):\n",
    "    '''Return list with parts of trajectories and their residuals.\n",
    "    \n",
    "    Arguments:\n",
    "        dset -- h5py Data set with trajectory data\n",
    "        keys -- keys for extracting data from `dset`\n",
    "        \n",
    "    Keyword arguments:\n",
    "        variables -- (default: 3) how many variables to extract:\n",
    "                     3 for XYZ -- coordinates,\n",
    "                     6 for XYZUVW -- coordinates and speeds,\n",
    "                     7 for XYZUVWH -- coords, speeds and altitude\n",
    "        \n",
    "        gt_size -- how many trajectory points are to be observed\n",
    "        step -- (default: 1) \n",
    "                if 1, every row from the `dset` will be processed,\n",
    "                if >1, some rows will be skipped accordingly.\n",
    "        diff -- (default: False) toggle extract differentiated relative trajectories\n",
    "    '''\n",
    "    assert gt_size < 1\n",
    "    assert variables in [3, 6, 7]\n",
    "    \n",
    "    # Create list with parts of trajectories,\n",
    "    # each element has gt_size trajectory points\n",
    "    data_seqs = []\n",
    "    \n",
    "    # Set of configs for each trajectory. \n",
    "    configs = {}\n",
    "    \n",
    "    # List of trajectory indices \n",
    "    # (to which trajectory this traj_elem belongs to)\n",
    "    traj_ids = []\n",
    "    \n",
    "    max_len = max_len or max(map(len, dset[keys]))\n",
    "    \n",
    "    # Collect trajectories, preprocess and \n",
    "    # split them into trajectory parts\n",
    "    for k in tqdm(range(len(keys)), disable=1-verbose, desc='Collecting strided data'):\n",
    "        # Get relative trajectory from the dataset\n",
    "        rt = RelativeTrajectory(dset[keys[k]], diff=diff)\n",
    "        configs[k] = rt.info()  # save for future restoration\n",
    "        \n",
    "        # Collect list of trajectory parts from `rt` \n",
    "        # using time window gt_size and time shift `step`\n",
    "        for i in range(1 + (rt.data.shape[0] - gt_size) // step):\n",
    "            data_seqs.append([rt.data[i*step : i*step + gt_size, :variables]])\n",
    "            traj_ids.append(k)\n",
    "    \n",
    "    # Collect all data seqs into one huge dataset \n",
    "    # of shape [? , gt_size, variables]\n",
    "    data_seqs_all = np.stack(data_seqs).squeeze()\n",
    "    traj_ids_all = np.stack(traj_ids)\n",
    "\n",
    "    # Calculate mean and std over all data\n",
    "    data_mean = data_seqs_all.mean((0, 1))\n",
    "    data_std = data_seqs_all.std((0, 1))\n",
    "    res_mean = np.zeros(variables)\n",
    "    res_std = np.ones(variables)\n",
    "    \n",
    "    \n",
    "    stats = {\n",
    "        'data_mean': data_mean,\n",
    "        'data_std': data_std,\n",
    "        'res_mean': res_mean,\n",
    "        'res_std': res_std\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        with np.printoptions(precision=3):\n",
    "            print(f'Total trajectory parts: {data_seqs_all.shape[0]}')\n",
    "            print(f'Each: {gt_size} observed = {gt_size} points in total')\n",
    "            print(f'Each point contains {data_seqs_all.shape[-1]} variables')\n",
    "\n",
    "            print('Data mean:', stats['data_mean'],\n",
    "                  'Data std:', stats['data_std'], \n",
    "                  sep='\\n')\n",
    "            \n",
    "    return (\n",
    "        data_seqs_all.squeeze()[:, :gt_size], # src sequences\n",
    "        data_seqs_all.squeeze()[:, gt_size:], # tgt sequences\n",
    "        configs,\n",
    "        stats,\n",
    "        traj_ids_all\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, data, name, variables=3, configs=None, stats=None):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.data = data\n",
    "        self.name = name\n",
    "        self.variables = variables\n",
    "        self.configs = configs\n",
    "        self.stats = stats\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data['src'].shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        read file_data\n",
    "        \"\"\"\n",
    "        ret =  {\n",
    "            'src': torch.Tensor(self.data['src'][index]),\n",
    "            'tgt': torch.Tensor(self.data['tgt'][index]),\n",
    "            'traj_id': self.data['traj_ids'][index],\n",
    "        }\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dset, keys, name, variables=3, gt_size=0, step=1, diff=False, train=True, scaler=None, verbose=False):\n",
    "    \"\"\"Create TrajectoryDataset for training NNs.\n",
    "\n",
    "    Arguments:\n",
    "        dset -- h5py dataset object\n",
    "        keys -- list of strings: keys for extracting data from `dset`\n",
    "        name -- name of the TrajectoryDataset\n",
    "\n",
    "    Keyword arguments:\n",
    "        variables -- (default: 3) how many variables to extract:\n",
    "                     3 for XYZ -- coordinates,\n",
    "                     6 for XYZUVW -- coordinates and speeds,\n",
    "                     7 for XYZUVWH -- coords, speeds and altitude\n",
    "\n",
    "        gt_size -- how many points are observed (model input)\n",
    "        step -- stride step for data\n",
    "        diff -- toggle differentiate trajectories\n",
    "        train -- this data will be used for training\n",
    "        scaler -- custom scaler, so data will have zero mean and unit variance\n",
    "        verbose -- toggle print info to the terminal\n",
    "        \n",
    "    Note:\n",
    "        If `train == True`, the scaler will fit on the collected data and\n",
    "        then returned as the TrajectoryDataset.scaler attribute\n",
    "        \n",
    "        If 'train == False', this function will look for scaler from the \n",
    "        arguments, then use it to scale collected data for evaluation.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"Loading dataset in {'train' if train else 'evaluation'} mode...\")\n",
    "\n",
    "    inp, out, configs, stats, traj_ids = get_strided_data_clust(\n",
    "        dset, keys, variables, gt_size, step, diff, verbose)\n",
    "\n",
    "    data = {\n",
    "        'src': inp,\n",
    "        'tgt': out, \n",
    "        'traj_ids': traj_ids\n",
    "    }\n",
    "\n",
    "    return TrajectoryDataset(data, name, variables, configs, stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(458, 162, '/ballistic_raw/BALLISTIC_665', '/ballistic_raw/BALLISTIC_264')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len_trajectory, min_len_trajectory, key_max, key_min = max_min_len_coordinate(dset, dset.keys())\n",
    "max_len_trajectory, min_len_trajectory, key_max, key_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< 66b963cdbadc79283eacff0a3febe18a2e108677
    "max_len_trajectory, key_max_trajectory = max_len_coordinate(dset, dset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "458"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len_trajectory"
=======
    "VEC_DIM = 3\n",
    "BATCH_SIZE = 15\n",
    "MEM_RNN = 64"
>>>>>>> A1.6macT4
   ]
  },
  {
   "cell_type": "code",
<<<<<<< 66b963cdbadc79283eacff0a3febe18a2e108677
   "execution_count": 22,
=======
   "execution_count": 14,
>>>>>>> A1.6macT4
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset in train mode...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
<<<<<<< 66b963cdbadc79283eacff0a3febe18a2e108677
       "model_id": "fd92f9cb33194628b1b9e70e9923cd0a",
=======
       "model_id": "c402685fc0df42d6ab73b76619456685",
>>>>>>> A1.6macT4
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Collecting strided data', max=700.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all input arrays must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-4bd2faf37234>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m td = create_dataset(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvariables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-8e2d203051b5>\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(dset, keys, name, variables, gt_size, step, diff, train, scaler, verbose)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading dataset in {'train' if train else 'evaluation'} mode...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     inp, out, configs, stats, traj_ids = get_strided_data_clust(\n\u001b[0m\u001b[1;32m     33\u001b[0m         dset, keys, variables, gt_size, step, diff, verbose)\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-a483f9d994fb>\u001b[0m in \u001b[0;36mget_strided_data_clust\u001b[0;34m(dset, keys, variables, gt_size, step, diff, verbose)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# Collect all data seqs into one huge dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# of shape [? , gt_size, variables]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mdata_seqs_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_seqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mtraj_ids_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraj_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all input arrays must have the same shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0mresult_ndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all input arrays must have the same shape"
     ]
    }
   ],
   "source": [
    "td = create_dataset(\n",
    "    dset,\n",
    "    train_keys,\n",
    "    name='train',\n",
    "    gt_size = max_len_trajectory,\n",
<<<<<<< 66b963cdbadc79283eacff0a3febe18a2e108677
    "    variables=3,\n",
=======
    "    step=max_len_trajectory,\n",
    "    variables=VEC_DIM,\n",
>>>>>>> A1.6macT4
    "    train=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< 66b963cdbadc79283eacff0a3febe18a2e108677
   "execution_count": 16,
=======
   "execution_count": 15,
>>>>>>> A1.6macT4
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    td,\n",
<<<<<<< 66b963cdbadc79283eacff0a3febe18a2e108677
    "    batch_size=1,\n",
=======
    "    batch_size=BATCH_SIZE,\n",
>>>>>>> A1.6macT4
    "    shuffle=False, #try  True\n",
    "    num_workers=0) #num_workers=-2# use CPU"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< 66b963cdbadc79283eacff0a3febe18a2e108677
   "execution_count": 17,
=======
   "execution_count": 16,
>>>>>>> A1.6macT4
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': tensor([[[0.0000, 0.0000, 0.0000],\n",
      "         [0.6965, 1.6882, 0.3880],\n",
      "         [1.3134, 3.1827, 0.7308],\n",
<<<<<<< 66b963cdbadc79283eacff0a3febe18a2e108677
      "         [1.8738, 4.5390, 1.0409],\n",
      "         [2.3938, 5.7961, 1.3266],\n",
      "         [2.8844, 6.9807, 1.5938],\n",
      "         [3.3535, 8.1115, 1.8462],\n",
      "         [3.8068, 9.2020, 2.0864]]]), 'tgt': tensor([], size=(1, 0, 3)), 'traj_id': tensor([0])}\n"
=======
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000],\n",
      "         [0.6365, 1.5376, 0.3536],\n",
      "         [1.1330, 2.7361, 0.6286],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000],\n",
      "         [0.7180, 1.7455, 0.4010],\n",
      "         [1.3880, 3.3735, 0.7744],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000],\n",
      "         [0.6803, 1.6478, 0.3788],\n",
      "         [1.2618, 3.0553, 0.7017],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000],\n",
      "         [0.6392, 1.5413, 0.3545],\n",
      "         [1.1386, 2.7445, 0.6307],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000],\n",
      "         [0.6216, 1.4994, 0.3449],\n",
      "         [1.0921, 2.6334, 0.6051],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000]]]), 'traj_id': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])}\n"
>>>>>>> A1.6macT4
     ]
    }
   ],
   "source": [
    "for id_b, batch in enumerate(train_loader):\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< 66b963cdbadc79283eacff0a3febe18a2e108677
   "execution_count": 13,
=======
   "execution_count": 17,
>>>>>>> A1.6macT4
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_mean': array([ 48.57584802,  65.9745059 , -68.19618657]),\n",
       " 'data_std': array([32.32703058, 46.89959796, 65.89417772]),\n",
       " 'res_mean': array([0., 0., 0.]),\n",
       " 'res_std': array([1., 1., 1.])}"
      ]
     },
<<<<<<< 66b963cdbadc79283eacff0a3febe18a2e108677
     "execution_count": 13,
=======
     "execution_count": 17,
>>>>>>> A1.6macT4
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td.stats"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< 66b963cdbadc79283eacff0a3febe18a2e108677
   "execution_count": 14,
=======
   "execution_count": 18,
>>>>>>> A1.6macT4
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< 66b963cdbadc79283eacff0a3febe18a2e108677
       "<class 'pandas.io.pytables.HDFStore'>\n",
       "File path: ./data/ballistic_batch_1000_nopad.h5"
      ]
     },
     "execution_count": 14,
=======
       "torch.Size([15, 458, 3])"
      ]
     },
     "execution_count": 18,
>>>>>>> A1.6macT4
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< 66b963cdbadc79283eacff0a3febe18a2e108677
    "dset"
=======
    "batch['src'].size()"
>>>>>>> A1.6macT4
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our recurrent block\n",
    "class ModuleRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Implement the scheme above as torch module\n",
    "    torch style\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, vect_dim=3, rnn_num_units=64):\n",
    "        super(self.__class__,self).__init__()\n",
    "        self.num_units = rnn_num_units\n",
    "        \n",
    "#         our linear layer\n",
    "        self.rnn_update = nn.Linear(vect_dim + rnn_num_units, rnn_num_units)\n",
    "        self.rnn_to_logits = nn.Linear(rnn_num_units, vect_dim)\n",
    "        \n",
    "    def forward(self, x, h_prev):\n",
    "        \"\"\"\n",
    "        This method computes h_next(x, h_prev) and log P(x_next | h_next)\n",
    "        We'll call it repeatedly to produce the whole sequence.\n",
    "        \n",
    "        :param x: batch of character ids, containing vector of int64\n",
    "        :param h_prev: previous rnn hidden states, containing matrix [batch, rnn_num_units] of float32\n",
    "        \"\"\"\n",
    "        \n",
    "        x_and_h = torch.cat([x, h_prev], dim=-1) # [x, h_prev]\n",
    "        h_next = self.rnn_update(x_and_h)\n",
    "        \n",
    "        h_next = torch.tanh(h_next) # activation function\n",
    "        \n",
    "        assert h_next.size() == h_prev.size()\n",
    "        \n",
    "        #compute logits for next character probs\n",
    "        logits = self.rnn_to_logits(h_next) \n",
    "        \n",
    "        return h_next, logits\n",
    "    \n",
    "    def initial_state(self, batch_size):\n",
    "        \"\"\" return rnn state before it processes first input (aka h0) \"\"\"\n",
    "        return torch.zeros(batch_size, self.num_units, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_loop(trajectories_rnn, batch_ix):\n",
    "    \"\"\"\n",
    "    Computes log P(next_character) for all time-steps in names_ix\n",
    "    :param names_ix: an int32 matrix of shape [batch, time], output of to_matrix(names)\n",
    "    \"\"\"\n",
    "    batch_size, max_length, vec_dim = batch_ix.size()\n",
    "    hid_state = trajectories_rnn.initial_state(batch_size)\n",
    "    logprobs = []\n",
    "\n",
    "    for x_t in batch_ix.transpose(0,1):\n",
    "        hid_state, logits = trajectories_rnn(x_t, hid_state)  # <-- here we call your one-step code\n",
    "        logprobs.append(logits)\n",
    "        \n",
    "    return torch.stack(logprobs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories_rnn = ModuleRNN(vect_dim=VEC_DIM, rnn_num_units=MEM_RNN)\n",
    "criterion = nn.MSELoss() # nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(trajectories_rnn.parameters(), lr=1e-3) # lr=1e-4\n",
    "history = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logp_seq = rnn_loop(trajectories_rnn, batch['src'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 3 #vect_dim\n",
    "\n",
    "# epoch\n",
    "for id_b, batch in enumerate(train_loader):\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    \n",
    "    logp_seq = rnn_loop(char_rnn, batch)\n",
    "    \n",
    "    # compute loss\n",
    "    predictions_logp = logp_seq[:, :-1] \n",
    "    actual_next_tokens = batch[:, 1:]\n",
    "    \n",
    "#     print(predictions_logp.shape, actual_next_tokens.shape)\n",
    "    loss = criterion(\n",
    "        predictions_logp.contiguous().view(-1, num_tokens),\n",
    "        actual_next_tokens.contiguous().view(-1)\n",
    "    ) \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # train with backprop\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    history.append(loss.data.numpy())\n",
    "    if (i+1)%100==0:\n",
    "        clear_output(True)\n",
    "        plt.plot(history,label='loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
