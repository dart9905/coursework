{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\\n\",\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# work with data\\n\",\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# like numpy, only good + NN\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as torch_nn_F\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "# import torchvision\n",
    "# from torchvision import transforms\n",
    "\n",
    "plt.rcParams[\n",
    "    \"figure.facecolor\"\n",
    "] = \"w\"  # force white background on plots when using dark mode in JupyterLab\n",
    "\n",
    "# Dark plots\\n\",\n",
    "#plt.style.use('dark_background')\\n\",\n",
    "\n",
    "TIME_STEP = 1\n",
    "CLASSNAME = {0: 'ballistic', 1: 'hgv', 2: 'hcm'}\n",
    "CLASSTYPE = {'ballistic': 0, 'hgv': 1, 'hcm': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data\n",
    "## Load and examine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data reading function\n",
    "def read_trajectories(path_to_file):\n",
    "    dset = pd.HDFStore(path_to_file, 'r')\n",
    "#     print(dset.info())\\n\",\n",
    "    return dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading\n",
    "name_file_trajectories = './data/ballistic_batch_1000_nopad.h5'\n",
    "try:\n",
    "    dset = read_trajectories(name_file_trajectories)\n",
    "except FileNotFoundError:\n",
    "    assert \"NO file!!!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(data, tts = (0.7, 0.1, 0.2), shuffle=False):\n",
    "    '''Split data into train, validation and test sets according to `tts` tuple\n",
    "\n",
    "    By default, tts = (train, val, test) = (0.7, 0.1, 0.2)\n",
    "    '''\n",
    "    assert sum(tts) == 1\n",
    "\n",
    "    if shuffle:\n",
    "        data = np.random.shuffle(data)\n",
    "\n",
    "    h = len(data)\n",
    "    train = data[:int(h * tts[0])]\n",
    "    val = data[int(h * tts[0]) : int(h * np.round(tts[0] + tts[1], 4))]\n",
    "    test = data[int(h * np.round(tts[0] + tts[1], 4)) : int(h * sum(tts))]\n",
    "\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train keys:   700 -> 70.0%\n",
      "Valid keys:   100 -> 10.0%\n",
      "Test keys:    200 -> 20.0%\n"
     ]
    }
   ],
   "source": [
    "# Train-val-test split data\n",
    "data_keys = sorted([key for key in dset.keys() if 'raw' in key])\n",
    "# Test data       = 20% of total keys\n",
    "# Validation data = 10% of total keys\n",
    "# Train data      = 70% of total keys\n",
    "train_keys, val_keys, test_keys = train_val_test_split(\n",
    "    data_keys, (0.7, 0.1, 0.2), shuffle=False)\n",
    "\n",
    "print(f'Train keys: {len(train_keys):>5} -> {len(train_keys)/len(data_keys) * 100}%')\n",
    "print(f'Valid keys: {len(val_keys):>5} -> {len(val_keys)/len(data_keys) * 100}%')\n",
    "print(f'Test keys:  {len(test_keys):>5} -> {len(test_keys)/len(data_keys) * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example\n",
    "# len(dset.keys())\n",
    "# dset[dset.keys()[0]]\n",
    "# len(np.array(dset[dset.keys()[0]])[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invertible trajectory pre-processing transform\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeTrajectory():\n",
    "    def __init__(self, df, diff=False, ref_point=20):\n",
    "\n",
    "        self.diff = diff\n",
    "        self.ref_point = ref_point\n",
    "        \n",
    "        data = np.array(df.drop('t', 1))\n",
    "        \n",
    "        # Remember first state\n",
    "        self.start_state = data[0].copy()\n",
    "        \n",
    "        # Make changes relative to the start state\n",
    "        data -= self.start_state\n",
    "        \n",
    "        # Rotate coordinate system around Z axis, \n",
    "        # so X` axis will pass through the 20'th point\n",
    "        # and Y` axis will represent deviation\n",
    "\n",
    "        ## Calculate angle of rotation: arctan(y_r / x_r), where r is ref_point\n",
    "        assert data.shape[0] >= ref_point\n",
    "        self.theta = np.arctan(data[ref_point][1] / data[ref_point][0])\n",
    "\n",
    "        ## Rotation matrix for XY plane around Z axis\n",
    "        ## Perform rotation for coordinates\n",
    "        data[:, :3] = self.rotate_Z(data[:, :3], self.theta)\n",
    "\n",
    "        ## Perform rotation for velocities\n",
    "        data[:, 3:6] = self.rotate_Z(data[:, 3:6], self.theta)\n",
    "\n",
    "        # Scale data to kilometers\n",
    "        data /= 1000\n",
    "\n",
    "        self.first_diff_elem = None\n",
    "        if diff:\n",
    "            self.first_diff_elem = data[0].copy()\n",
    "            data = np.diff(data, axis=0)    \n",
    "        \n",
    "        self.data = data\n",
    "        \n",
    "    def restore(self, columns, ts=1, config=None):\n",
    "        \n",
    "        assert ts > 0\n",
    "        \n",
    "        if config:\n",
    "            self.set_info(config)\n",
    "        \n",
    "        # Restore diff data\n",
    "        if self.diff:\n",
    "            data = np.r_[np.expand_dims(self.first_diff_elem, 0), np.cumsum(self.data, axis=0)]\n",
    "        else:         \n",
    "            data = self.data.copy()\n",
    "        \n",
    "        # Scale data from km back to meters\n",
    "        data *= 1000\n",
    "        \n",
    "        ## Rotation matrix for XY plane around Z axis\n",
    "        ## Perform rotation for coordinates\n",
    "        data[:, 0:3] = self.rotate_Z(data[:, 0:3], -self.theta)\n",
    "\n",
    "        ## Perform rotation for velocities\n",
    "        data[:, 3:6] = self.rotate_Z(data[:, 3:6], -self.theta)\n",
    "        \n",
    "        # Make changes absolute\n",
    "        data += self.start_state\n",
    "\n",
    "        # Restore Pandas.DataFrame format\n",
    "        t = np.arange(0, data.shape[0], ts)\n",
    "        data = np.c_[data, t] \n",
    "        data = pd.DataFrame(data, columns=columns)\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    @staticmethod\n",
    "    def rotate_Z(data, theta):\n",
    "        \"\"\"Rotate data around the Z axis using matrix R\"\"\"\n",
    "        \n",
    "        R = np.array([\n",
    "            [np.cos(theta), -np.sin(theta), 0],\n",
    "            [np.sin(theta),  np.cos(theta), 0],\n",
    "            [            0,              0, 1]\n",
    "        ])\n",
    "        return data @ R.T\n",
    "    \n",
    "    def info(self):\n",
    "        return {\n",
    "            'ref_point' : self.ref_point,\n",
    "            'diff' : self.diff,\n",
    "            'start_state' : self.start_state,\n",
    "            'first_diff_elem' : self.first_diff_elem,\n",
    "            'theta' : self.theta\n",
    "        }\n",
    "    \n",
    "    def set_info(self, config):\n",
    "        self.ref_point = config['ref_point']\n",
    "        self.diff = config['diff']\n",
    "        self.start_point = config['start_point']\n",
    "        self.first_diff_elem = config['first_diff_elem']\n",
    "        self.theta = config['theta']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_min_len_coordinate(dset, keys):\n",
    "    max_size = 0\n",
    "    min_size = len(np.array(dset[keys[0]])[:,0])\n",
    "    for key_k in keys:\n",
    "        size = len(np.array(dset[key_k])[:,0])\n",
    "        if size > max_size:\n",
    "            max_size = size\n",
    "            max_key = key_k\n",
    "        if size < min_size:\n",
    "            min_size = size\n",
    "            min_key = key_k\n",
    "    return (max_size, min_size, max_key, min_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_strided_data_clust(dset, keys, variables=3, gt_size=0, step=1, diff=False, verbose=False):\n",
    "    '''Return list with parts of trajectories and their residuals.\n",
    "\n",
    "    Arguments:\n",
    "    dset -- h5py Data set with trajectory data\n",
    "        keys -- keys for extracting data from `dset`\n",
    "\n",
    "    Keyword arguments:\n",
    "    variables -- (default: 3) how many variables to extract:\n",
    "                     3 for XYZ -- coordinates,\n",
    "                     6 for XYZUVW -- coordinates and speeds,\n",
    "                     7 for XYZUVWH -- coords, speeds and altitude\n",
    "\n",
    "        gt_size -- how many trajectory points are to be observed\n",
    "        step -- (default: 1)\n",
    "                if 1, every row from the `dset` will be processed,\n",
    "                if >1, some rows will be skipped accordingly.\n",
    "        diff -- (default: False) toggle extract differentiated relative trajectories\n",
    "    '''\n",
    "    assert gt_size > 1\n",
    "    assert variables in [3, 6, 7]\n",
    "\n",
    "    # Create list with parts of trajectories,\n",
    "    # each element has gt_size trajectory points\n",
    "    data_seqs = []\n",
    "\n",
    "    # Set of configs for each trajectory.\n",
    "    configs = {}\n",
    "\n",
    "    # List of trajectory indices\n",
    "    # (to which trajectory this traj_elem belongs to)\n",
    "    traj_ids = []\n",
    "\n",
    "\n",
    "    # Collect trajectories, preprocess and\n",
    "    # split them into trajectory parts\n",
    "    for k in tqdm(range(len(keys)), disable=1-verbose, desc='Collecting strided data'):\n",
    "        # Get relative trajectory from the dataset\n",
    "        rt = RelativeTrajectory(dset[keys[k]], diff=diff)\n",
    "        configs[k] = rt.info()  # save for future restoration\n",
    "\n",
    "        # Collect list of trajectory parts from `rt`\n",
    "        # using time window gt_size and time shift `step`\n",
    "        if gt_size < rt.data.shape[0]:\n",
    "            for i in range(1 + (rt.data.shape[0] - gt_size) // step):\n",
    "                data_seqs.append([rt.data[i*step : i*step + gt_size, :variables]])\n",
    "                traj_ids.append(k)\n",
    "        else:\n",
    "            new_rt = np.zeros((gt_size,variables))  \n",
    "            new_rt[0:rt.data.shape[0],0:variables] = rt.data[:, :variables]\n",
    "            data_seqs.append([new_rt])\n",
    "            traj_ids.append(k)\n",
    "            \n",
    "    # Collect all data seqs into one huge dataset\n",
    "    # of shape [? , gt_size, variables]\n",
    "    data_seqs_all = np.stack(data_seqs).squeeze()\n",
    "    traj_ids_all = np.stack(traj_ids)\n",
    "\n",
    "    # Calculate mean and std over all data\n",
    "    data_mean = data_seqs_all.mean((0, 1))\n",
    "    data_std = data_seqs_all.std((0, 1))\n",
    "    res_mean = np.zeros(variables)\n",
    "    res_std = np.ones(variables)\n",
    "    \n",
    "    \n",
    "    stats = {\n",
    "        'data_mean': data_mean,\n",
    "        'data_std': data_std,\n",
    "        'res_mean': res_mean,\n",
    "        'res_std': res_std\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        with np.printoptions(precision=3):\n",
    "            print(f'Total trajectory parts: {data_seqs_all.shape[0]}')\n",
    "            print(f'Each: {gt_size} observed = {gt_size} points in total')\n",
    "            print(f'Each point contains {data_seqs_all.shape[-1]} variables')\n",
    "\n",
    "            print('Data mean:', stats['data_mean'],\n",
    "                  'Data std:', stats['data_std'],\n",
    "                  sep='\\\\n')\n",
    "\n",
    "    return (\n",
    "        data_seqs_all.squeeze()[:, :gt_size], # src sequences\n",
    "        configs,\n",
    "        stats,\n",
    "        traj_ids_all\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, name, variables=3, configs=None, stats=None):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.data = data\n",
    "        self.name = name\n",
    "        self.variables = variables\n",
    "        self.configs = configs\n",
    "        self.stats = stats\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data['src'].shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        ret =  {\n",
    "            'src': torch.Tensor(self.data['src'][index]),\n",
    "            'traj_id': self.data['traj_ids'][index],\n",
    "        }\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dset, keys, name, variables=3, gt_size=8, step=1, diff=False, train=True, scaler=None, verbose=False):\n",
    "    \"\"\"Create TrajectoryDataset for training NNs.\n",
    "\n",
    "    Arguments:\n",
    "        dset -- h5py dataset object\n",
    "        keys -- list of strings: keys for extracting data from `dset`\n",
    "        name -- name of the TrajectoryDataset\n",
    "\n",
    "    Keyword arguments:\n",
    "        variables -- (default: 3) how many variables to extract:\n",
    "                     3 for XYZ -- coordinates,\n",
    "                     6 for XYZUVW -- coordinates and speeds,\n",
    "                     7 for XYZUVWH -- coords, speeds and altitude\n",
    "\n",
    "        residuals -- (default: True) if True, residuals of trajectory will \n",
    "                     be concatenated, such that, in case of 3 variables: \n",
    "                     [[X1, Y1, Z1],      [[X1, Y1, Z1,     0,     0,     0],\n",
    "                      [X2, Y2, Z2],  -->  [X2, Y2, Z2, X2-X1, Y2-Y1, Z2-Z1],\n",
    "                      [X3, Y3, Z3]]       [X3, Y3, Z3, X3-X2, Y3-Y2, Z3-Z2]]\n",
    "\n",
    "        gt_size -- how many points are observed (model input)\n",
    "        horizon -- how many points the model tries to predict into the future\n",
    "        step -- stride step for data\n",
    "        diff -- toggle differentiate trajectories\n",
    "        train -- this data will be used for training\n",
    "        scaler -- custom scaler, so data will have zero mean and unit variance\n",
    "        verbose -- toggle print info to the terminal\n",
    "        \n",
    "    Note:\n",
    "        If `train == True`, the scaler will fit on the collected data and\n",
    "        then returned as the TrajectoryDataset.scaler attribute\n",
    "        \n",
    "        If 'train == False', this function will look for scaler from the \n",
    "        arguments, then use it to scale collected data for evaluation.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"Loading dataset in {'train' if train else 'evaluation'} mode...\")\n",
    "\n",
    "    inp, configs, stats, traj_ids = get_strided_data_clust(\n",
    "        dset, keys, variables, gt_size, step, diff, verbose)\n",
    "\n",
    "    data = {\n",
    "        'src': inp,\n",
    "        'traj_ids': traj_ids\n",
    "    }\n",
    "\n",
    "    return TrajectoryDataset(data, name, variables, configs, stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(458, 162, '/ballistic_raw/BALLISTIC_665', '/ballistic_raw/BALLISTIC_264')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len_trajectory, min_len_trajectory, key_max, key_min = max_min_len_coordinate(dset, dset.keys())\n",
    "max_len_trajectory, min_len_trajectory, key_max, key_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "VEC_DIM = 3\n",
    "BATCH_SIZE = 15\n",
    "MEM_RNN = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset in train mode...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70e2bad3a3db439c84afd6fa0987c702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Collecting strided data', max=700.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total trajectory parts: 700\n",
      "Each: 458 observed = 458 points in total\n",
      "Each point contains 3 variables\n",
      "Data mean:\\n[ 26.875  35.61  -39.398]\\nData std:\\n[33.848 47.206 61.776]\n"
     ]
    }
   ],
   "source": [
    "td = create_dataset(\n",
    "    dset,\n",
    "    train_keys,\n",
    "    name='train',\n",
    "    gt_size = max_len_trajectory,\n",
    "    step=max_len_trajectory,\n",
    "    variables=VEC_DIM,\n",
    "    train=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(\n",
    "    td,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False, #try  True\\n\",\n",
    "    num_workers=0) #num_workers=-2# use CPU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': tensor([[[0.0000, 0.0000, 0.0000],\n",
      "         [0.6965, 1.6882, 0.3880],\n",
      "         [1.3134, 3.1827, 0.7308],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000],\n",
      "         [0.6365, 1.5376, 0.3536],\n",
      "         [1.1330, 2.7361, 0.6286],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000],\n",
      "         [0.7180, 1.7455, 0.4010],\n",
      "         [1.3880, 3.3735, 0.7744],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000],\n",
      "         [0.6803, 1.6478, 0.3788],\n",
      "         [1.2618, 3.0553, 0.7017],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000],\n",
      "         [0.6392, 1.5413, 0.3545],\n",
      "         [1.1386, 2.7445, 0.6307],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000],\n",
      "         [0.6216, 1.4994, 0.3449],\n",
      "         [1.0921, 2.6334, 0.6051],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000]]]), 'traj_id': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])}\n"
     ]
    }
   ],
   "source": [
    "for id_b, batch in enumerate(train_dl):\n",
    "    \n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 458, 3])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['src'].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our recurrent block\n",
    "class ModuleRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Implement the scheme above as torch module\n",
    "    torch style\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, vect_dim=3, rnn_num_units=64):\n",
    "        super(self.__class__,self).__init__()\n",
    "        self.num_units = rnn_num_units\n",
    "\n",
    "#         our linear layer\n",
    "        self.rnn_update = nn.Linear(vect_dim + rnn_num_units, rnn_num_units)\n",
    "        self.rnn_to_logits = nn.Linear(rnn_num_units, vect_dim)\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        \"\"\"\n",
    "        This method computes h_next(x, h_prev) and log P(x_next | h_next)\n",
    "        We'll call it repeatedly to produce the whole sequence.\n",
    "\n",
    "        :param x: batch of character ids, containing vector of int64\n",
    "        :param h_prev: previous rnn hidden states, containing matrix [batch, rnn_num_units] of float32\n",
    "        \"\"\"\n",
    "\n",
    "        x_and_h = torch.cat([x, h_prev], dim=-1) # [x, h_prev]\n",
    "        h_next = self.rnn_update(x_and_h)\n",
    "\n",
    "        h_next = torch.tanh(h_next) # activation function\n",
    "\n",
    "        assert h_next.size() == h_prev.size()\n",
    "\n",
    "        #compute logits for next character probs\n",
    "        logits = self.rnn_to_logits(h_next)\n",
    "\n",
    "        return h_next, logits\n",
    "\n",
    "    def initial_state(self, batch_size):\n",
    "        \"\"\" return rnn state before it processes first input (aka h0) \"\"\"\n",
    "        return torch.zeros(batch_size, self.num_units, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_loop(trajectories_rnn, batch_ix):\n",
    "    \"\"\"\n",
    "    Computes log P(next_character) for all time-steps in names_ix\n",
    "    :param names_ix: an int32 matrix of shape [batch, time], output of to_matrix(names)\n",
    "    \"\"\"\n",
    "    batch_size, max_length, vec_dim = batch_ix.size()\n",
    "\n",
    "    hid_state = trajectories_rnn.initial_state(batch_size)\n",
    "    logprobs = []\n",
    "\n",
    "    for x_t in batch_ix.transpose(0,1):\n",
    "        hid_state, logits = trajectories_rnn(x_t, hid_state)  # <-- here we call your one-step code\n",
    "        logprobs.append(logits)\n",
    "\n",
    "    return torch.stack(logprobs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories_rnn = ModuleRNN(vect_dim=VEC_DIM, rnn_num_units=MEM_RNN)\n",
    "riterion = nn.MSELoss() # nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(trajectories_rnn.parameters(), lr=1e-3) # lr=1e-4\n",
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logp_seq = rnn_loop(trajectories_rnn, batch['src'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 3 #vect_dim\n",
    "\n",
    "# epoch\n",
    "for i in range(1000):\n",
    "    opt.zero_grad()\n",
    "\n",
    "    batch_ix = to_matrix(sample(names, 32), max_len=MAX_LENGTH)\n",
    "    batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
    "\n",
    "    logp_seq = rnn_loop(char_rnn, batch_ix)\n",
    "\n",
    "    # compute loss\n",
    "    predictions_logp = logp_seq[:, :-1] \n",
    "    actual_next_tokens = batch_ix[:, 1:] \n",
    "\n",
    "#     print(predictions_logp.shape, actual_next_tokens.shape)\n",
    "    loss = criterion(\n",
    "        predictions_logp.contiguous().view(-1, num_tokens),\n",
    "        actual_next_tokens.contiguous().view(-1)\n",
    "    ) \n",
    "\n",
    "    \n",
    "    # train with backprop\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "\n",
    "    \n",
    "    history.append(loss.data.numpy())\n",
    "    if (i+1)%100==0:\n",
    "        clear_output(True)\n",
    "        plt.plot(history,label='loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
